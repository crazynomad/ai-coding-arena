# 🏟️ AI Coding Arena

> 用日常真实编程需求，测试不同 AI 模型的 Agentic Coding 能力

## 为什么做这个？

大模型评测总是用学术 benchmark，离普通开发者太远。这个项目用**真实、接地气的日常编程任务**来测试 AI 模型，看看它们到底能不能帮你干活。

## 测试方式

每个挑战都是一个独立目录，包含：

- `PROMPT.md` — 给 AI 的原始提示词
- `starter/` — 起始文件（如果有的话）
- `results/<model>/` — 各模型的生成结果
- `VERDICT.md` — 评测结论和对比

## 挑战列表

### 🟢 入门级
| # | 挑战 | 描述 | 状态 |
|---|------|------|------|
| 01 | CLI 工具 | 写一个实用的命令行工具（如文件批量重命名） | ⬜ |
| 02 | REST API | 从零搭建一个带 CRUD 的 REST API | ⬜ |
| 03 | 数据可视化 | 用真实数据生成交互式图表页面 | ⬜ |

### 🟡 进阶级
| # | 挑战 | 描述 | 状态 |
|---|------|------|------|
| 04 | Bug 猎手 | 给一段有 bug 的代码，让 AI 找出并修复 | ⬜ |
| 05 | 重构大师 | 把一坨屎山代码重构成优雅的版本 | ⬜ |
| 06 | 全栈小应用 | 前后端一起写，如记账本、TODO app | ⬜ |

### 🔴 硬核级
| # | 挑战 | 描述 | 状态 |
|---|------|------|------|
| 07 | 框架迁移 | 把一个项目从框架 A 迁移到框架 B | ⬜ |
| 08 | 性能优化 | 给一段慢代码，让 AI 优化到飞起 | ⬜ |
| 09 | 逆向工程 | 给一个 API 的输入输出，让 AI 还原实现 | ⬜ |

### 🎮 趣味级
| # | 挑战 | 描述 | 状态 |
|---|------|------|------|
| 10 | 网页游戏 | 生成一个可玩的浏览器游戏 | ⬜ |
| 11 | 创意页面 | 生成一个炫酷的个人主页 / Landing Page | ⬜ |
| 12 | 自动化脚本 | 写一个解决实际痛点的自动化脚本 | ⬜ |

## 参赛模型

| 模型 | 厂商 | 类型 |
|------|------|------|
| Claude Opus 4.6 | Anthropic | 闭源 |
| GPT-5.3-Codex | OpenAI | 闭源 |
| GLM-5 | 智谱 | 开源 |
| Gemini 2.5 Pro | Google | 闭源 |
| *更多待加...* | | |

## 评测维度

- ✅ **正确性** — 代码能跑吗？结果对吗？
- 🎨 **质量** — 代码风格、架构设计、可维护性
- ⚡ **效率** — 生成速度、token 消耗
- 🔄 **自主性** — 遇到错误能自己修吗？需要人工干预几次？
- 💡 **创意** — 有没有超出预期的惊喜？

## 如何复现

每个挑战目录下都有完整的提示词和起始文件，你可以：

1. 复制 `PROMPT.md` 的内容
2. 在你喜欢的 AI 编程工具里跑一遍
3. 对比结果，欢迎提 PR 补充

## 贡献

欢迎提交新的挑战题目或补充模型测试结果！详见 [CONTRIBUTING.md](CONTRIBUTING.md)

## License

MIT
